# ==========================================
# PREPROCESSING CONFIGURATION
# =========================================
# This file documents all preprocessing decisions based on EDA findings
# Every decision here is justified by EDA analysis
#
# Author : Maxwell Selassie Hiamatsu
# Date : 24th October, 2025
# Dataset : Retail store sales
# EDA Version : 1.0
# ==============================================

# -----------------------------------------
# PROJECT METADATA
# =======================================

project:
  name: 'Retail store sales'
  version: '1.0.0'
  description: 'E-commerce Retail sales data preprocessing pipeline'

# =======================================
# FILE PATHS
# ===========================================
paths:
  raw_data: 'data/raw/retail_store_sales.csv'
  processed_data: 'data/processed/retail_cleaned.csv'
  preprocessing_metadata: 'logs/preprocessing_metadata.json'
  scaler_path: 'models/scaler.pkl'
  encoder_path: 'models/encoder.pkl'

# ==================================
# COLUMNS TO DROP
# =================================
columns_to_drop:
  - column: 'Transaction ID'
    reason: High cardinality identifier (12,575 unique values). Not predictive.
    eda_evidence: cardinality analysis showed 12,575 unique values

# ====================================
# MISSING VALUES HANDLING
# =====================================
missing values: # strategy per column based on EDA findings
  numeric: # numeric columns

    Price Per Unit: 
      method: 'mean'
      reason: 'Only 4.84% missing. No outliers from boxplot, thus mean imputation'
      eda_evidence: 'missing_summary.csv shows 4.84% missing data. 
                  Histogram plot shows feature has no skewness and does not follow a normal distribution'

    Quantity: 
      method: 'mean'
      reason: 'Only 4.8% missing. No outliers from boxplot, hence, mean imputation'
      eda_evidence: 'missing_summary.csv shows 4.8% missing data.
                    Histogram plot shows feature has no skewness and does not follow a normal distribution'
      
    Total_spent: 
      method: 'calculated'
      reason: 'Can be derived from Price Per Unit * Quantity'
      eda_evidence: 'business_sanity_checks shows over 90% matches'

  categorical: # categorical columns
    Discount Applied:
      method: 'replace NaN with "not specified"'
      reason: 'The discount applied columns has 3 values (True, False, NaN). Replace with "Not Specified" and then one-hot encode'
      eda_evidence: 'missing_summary.csv shows 33.39% missing data, categorical columns show discount applied 
                    column has only two unique values excluding NaN'

    Item:
      method: 'calculated'
      reason: 'Can be derived from the "Category" column'
      eda_evidence: 'summary_overview shows missing values from Item column can be computed from Category column'

# Threshold for dropping columns 
column_drop_threshold: 0.7  # drop columns with 70% missing data

# ================================
# OUTLIER HANDLING
# ===============================

# based on IQR analysis from EDA
outliers:
  method: 'IQR'

# per-column strategies
  columns: 
    Total Spent: 
      action: 'none'
      reason: '48% outlier detected! All those outlier data values had just one unique value, hence, the decision to not make changes'
      eda_evidence: "outlier_summary.json shows 48% outliers. All have a value of 410 which isn't far from the upper range of 403.5"

iqr_multiplier: 1.5

# ===============================
# DATA QUALITY FIXES
# =================================
data_quality: 
# remove duplicate rows
  Total Spent:
    action: 'recalculate'
    reason: 'Should always be Price Per Unit * Quantity'
    eda_evidence: 'Business sanity checks shows about 10% mismatches'

  recalculate_columns:
    Total Spent:
      formula: 'Price Per Unit * Quantity'
      reason: Ensure consistency with business logic
      override_existing: false # only fill if missing or failed sanity checks

# ===========================================
# FEATURE ENCODING
# ==========================================
encoding:
  # one hot encoding (for low cardinality columns)  
  one_hot:
    - column: 'Category'
      max_categories: 10 # if more, use target encoding instead
      drop_first: True 
      reason: '8 categories, no ordinal relationships'
      eda_evidence: 'categorical columns showed 8 unique values'

    - column: 'Payment Method'
      max_categories: 10
      drop_first: True
      reason: '3 categories, no ordinal relationships'
      eda_evidence: 'categorical columns showed 3 unique values'

    - column : 'Location'
      max_categories: 10
      drop_first: True
      reason: '2 categories, no ordinal relationships'
      eda_evidence: 'categorical columns showed 2 unique values'

    - column: 'Discount Applied'
      max_categories: 10
      drop_first: True
      reason: '3 categories, after handling missing values'
      eda_evidence: 'categorical columns showed 2 unique values'

  # target encoding for high cardinality features 
  target:
    - column: 'Customer ID'
      smoothing: 1.0
      reason: '25 unique customer IDs. Target encoding captures customer performance'
      eda_evidence: 'categorical columns showed 25 unique values'

    - column: 'Item'
      smoothing: 1.0
      reason: '200 unique Items. Target encoding captures item performances'
      eda_evidence: 'categorical columns showed 200 unique values'

# ================================
# FEATURE SCALING
# =================================
scaling:
  method: 'standard'
# columns to scale

  columns:
    - 'Price Per Unit'
    - 'Quantity'
    - 'Total Spent'

  exclude:
    - 'Is_weekend'

  reason: "standard scaling for linear models. Tree-based models don't need scaling"
  fit_on: 'train'

# ================================
# DATETIME FEATURES
# ===================================
datetime:
  date_column:
    - 'Transaction Date' 

  date_format:
    - '%Y-%m-%d'

  extract_features:
    - 'year'
    - 'month'
    - 'day'
    - 'day_of_week'
    - 'quarter'
    - 'Is_weekend'
    - 'Is_month_start'
    - 'Is_month_end'

  drop_column: True # drop original column after extraction

  reason: 'To analyze temporal patterns in retail sales'

# =============================================
# LOGGING & MONITORING
# =============================================
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  save_transformations: true
  log_file: "logs/preprocessing.log"
  
  # Track statistics before/after each step
  track_statistics:
    - "shape"
    - "missing_values"
    - "memory_usage"
    - "numeric_distributions"

# ========================================
# NOTE AND ASSUMPTIONS
# ===========================================
notes:
  assumptions:
    - 'Missing values are Missing At Random'
    - 'Outliers are not extreme values' 
    - 'Transaction Date represent transaction time, not delivery time'

  limitations:
    - 'Customer demographics is only limited to "in-store and online"'
  
  future_improvements:
    - 'Add seasonality indicators'
    - 'Add customer demographics'
    
